#!/usr/bin/env python3
"""
Flash Attention Debug è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬ä¼šè¿è¡Œå¸¦æœ‰è¯¦ç»†è°ƒè¯•ä¿¡æ¯çš„Flash Attentionå®ç°ï¼Œ
å¸®åŠ©ä½ ç†è§£ç®—æ³•çš„æ¯ä¸ªæ­¥éª¤ã€‚
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
triton_dir = os.path.join(current_dir, 'triton')
sys.path.insert(0, current_dir)
sys.path.insert(0, triton_dir)

import torch

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
try:
    from flash_attention_debug import test_debug_attention
    DEBUG_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥debugæ¨¡å—: {e}")
    DEBUG_AVAILABLE = False

# å°è¯•å¯¼å…¥å¯è§†åŒ–æ¨¡å—
try:
    from visualization import create_learning_visualization
    VISUALIZATION_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨: {e}")
    VISUALIZATION_AVAILABLE = False

def main():
    print("ğŸ“ Flash Attention å­¦ä¹ å·¥å…·")
    print("=" * 60)
    print("è¿™ä¸ªå·¥å…·ä¼šå±•ç¤ºFlash Attentionçš„è¯¦ç»†æ‰§è¡Œè¿‡ç¨‹")
    print("åŒ…æ‹¬:")
    print("  â€¢ è¾“å…¥è¾“å‡ºå¼ é‡çš„å½¢çŠ¶å’Œæ•°å€¼èŒƒå›´")
    print("  â€¢ æ¯ä¸ªè®¡ç®—æ­¥éª¤çš„è¯´æ˜")
    print("  â€¢ ä¸æ ‡å‡†å®ç°çš„ç»“æœæ¯”è¾ƒ")
    print("  â€¢ æ ¸å¿ƒæ¦‚å¿µçš„æ€»ç»“")
    print("=" * 60)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯: éœ€è¦CUDAæ”¯æŒæ‰èƒ½è¿è¡ŒTriton")
        print("è¯·ç¡®ä¿:")
        print("  1. å®‰è£…äº†æ”¯æŒCUDAçš„PyTorch")
        print("  2. ç³»ç»Ÿæœ‰å¯ç”¨çš„GPU")
        return
    
    print(f"âœ… CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥æ¨¡å—å¯ç”¨æ€§
    if not DEBUG_AVAILABLE:
        print("âŒ Debugæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
        return
    
    try:
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦è¿è¡Œå¯è§†åŒ–
        if VISUALIZATION_AVAILABLE:
            print("\nğŸ¤” ä½ æƒ³è¦è¿è¡Œå“ªä¸ªéƒ¨åˆ†?")
            print("1. ä»…è¿è¡Œè°ƒè¯•ç‰ˆæœ¬ (å¿«é€Ÿ)")
            print("2. è¿è¡Œè°ƒè¯•ç‰ˆæœ¬ + å¯è§†åŒ– (éœ€è¦matplotlib)")
            print("3. ä»…è¿è¡Œå¯è§†åŒ–")
            
            choice = input("\nè¯·é€‰æ‹© (1/2/3, é»˜è®¤1): ").strip() or "1"
        else:
            print("\nâš ï¸  å¯è§†åŒ–åŠŸèƒ½ä¸å¯ç”¨ (ç¼ºå°‘matplotlib/seaborn)")
            print("å¦‚éœ€å¯è§†åŒ–ï¼Œè¯·è¿è¡Œ: pip install matplotlib seaborn")
            choice = "1"
        
        if choice in ["1", "2"]:
            # è¿è¡Œdebugæµ‹è¯•
            test_debug_attention()
        
        if choice in ["2", "3"] and VISUALIZATION_AVAILABLE:
            print("\nğŸ¨ å¼€å§‹å¯è§†åŒ–...")
            create_learning_visualization()
        
        print("\n" + "ğŸ‰" * 20)
        print("æ­å–œ! Flash Attentionå­¦ä¹ å®Œæˆ!")
        print("ğŸ‰" * 20)
        
        print("\nğŸ“š è¿›ä¸€æ­¥å­¦ä¹ å»ºè®®:")
        print("1. å°è¯•ä¿®æ”¹SEQ_LENå‚æ•°ï¼Œè§‚å¯Ÿæ€§èƒ½å·®å¼‚")
        print("2. æ¯”è¾ƒå› æœå’Œéå› æœæ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹")
        print("3. ç ”ç©¶ä¸åŒBLOCK_SIZEå¯¹å†…å­˜ä½¿ç”¨çš„å½±å“")
        print("4. é˜…è¯»Flash Attentionè®ºæ–‡äº†è§£ç†è®ºèƒŒæ™¯")
        print("5. è¿è¡Œå¯è§†åŒ–å·¥å…·ç†è§£åˆ†å—è®¡ç®—è¿‡ç¨‹")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥Tritonæ˜¯å¦æ­£ç¡®å®‰è£…: pip install triton")
        print("2. ç¡®ä¿GPUå†…å­˜è¶³å¤Ÿ")
        print("3. å°è¯•å‡å°æµ‹è¯•å‚æ•°ï¼ˆSEQ_LENç­‰ï¼‰")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
