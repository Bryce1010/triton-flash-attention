#!/usr/bin/env python3
"""
ç®€åŒ–çš„Flash Attentionè°ƒè¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬ä¸“æ³¨äºå±•ç¤ºFlash Attentionçš„æ ¸å¿ƒæ¦‚å¿µï¼Œ
é¿å…å¤æ‚çš„å¯¼å…¥å’Œæ•°æ®ç±»å‹é—®é¢˜ã€‚
"""

import torch
import sys
import os

# æ·»åŠ tritonç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'triton'))

def print_debug(msg, *args):
    """Debugæ‰“å°å‡½æ•°"""
    print(f"[DEBUG] {msg}", *args)

def print_tensor_info(name, tensor):
    """æ‰“å°å¼ é‡ä¿¡æ¯"""
    if tensor is not None:
        print(f"[TENSOR] {name}: shape={tensor.shape}, dtype={tensor.dtype}")
        if tensor.numel() < 20:
            print(f"         values={tensor.flatten()}")
        else:
            print(f"         min={tensor.min().item():.4f}, max={tensor.max().item():.4f}, mean={tensor.mean().item():.4f}")

def explain_flash_attention_concepts():
    """è§£é‡ŠFlash Attentionçš„æ ¸å¿ƒæ¦‚å¿µ"""
    print("\n" + "ğŸ“" * 20)
    print("Flash Attention æ ¸å¿ƒæ¦‚å¿µè¯¦è§£")
    print("ğŸ“" * 20)
    
    print("\n1ï¸âƒ£ æ ‡å‡†Attentionçš„é—®é¢˜:")
    print("   â€¢ å†…å­˜å¤æ‚åº¦: O(NÂ²) - éœ€è¦å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ")
    print("   â€¢ è®¡ç®—å¤æ‚åº¦: O(NÂ²) - å¯¹äºé•¿åºåˆ—éå¸¸æ˜‚è´µ")
    print("   â€¢ å†…å­˜è®¿é—®: é¢‘ç¹çš„HBMè®¿é—®ï¼Œæ•ˆç‡ä½")
    
    print("\n2ï¸âƒ£ Flash Attentionçš„è§£å†³æ–¹æ¡ˆ:")
    print("   â€¢ ğŸ§© åˆ†å—è®¡ç®—: å°†å¤§çŸ©é˜µåˆ†æˆå°å—å¤„ç†")
    print("   â€¢ ğŸ“Š åœ¨çº¿softmax: ä¸éœ€è¦å®Œæ•´çŸ©é˜µå°±èƒ½è®¡ç®—softmax")
    print("   â€¢ ğŸ’¾ å†…å­˜ä¼˜åŒ–: ä½¿ç”¨GPUçš„SRAMè€Œä¸æ˜¯HBM")
    print("   â€¢ âš¡ æµå¼å¤„ç†: è¾¹è®¡ç®—è¾¹ä¸¢å¼ƒä¸­é—´ç»“æœ")
    
    print("\n3ï¸âƒ£ åœ¨çº¿Softmaxç®—æ³•:")
    print("   â€¢ m_i: è¿è¡Œæœ€å¤§å€¼ (running maximum)")
    print("   â€¢ l_i: è¿è¡Œå’Œ (running sum)")
    print("   â€¢ Î±: ä¿®æ­£å› å­ = exp(m_old - m_new)")
    print("   â€¢ å…¬å¼: O_new = (O_old Ã— Î± + P Ã— V) / l_new")
    
    print("\n4ï¸âƒ£ åˆ†å—ç­–ç•¥:")
    print("   â€¢ Qå—: å›ºå®šä¸€ä¸ªQå—ï¼Œéå†æ‰€æœ‰K/Vå—")
    print("   â€¢ K/Vå—: é€ä¸ªåŠ è½½ï¼Œè®¡ç®—åä¸¢å¼ƒ")
    print("   â€¢ å› æœæ©ç : åªåœ¨å¯¹è§’çº¿å—åº”ç”¨æ©ç ")

def demonstrate_online_softmax():
    """æ¼”ç¤ºåœ¨çº¿softmaxç®—æ³•"""
    print("\n" + "ğŸ”¬" * 20)
    print("åœ¨çº¿Softmaxç®—æ³•æ¼”ç¤º")
    print("ğŸ”¬" * 20)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¾‹å­
    torch.manual_seed(42)
    x = torch.tensor([1.0, 3.0, 2.0, 4.0])
    print(f"è¾“å…¥å‘é‡: {x}")
    
    # æ ‡å‡†softmax
    standard_softmax = torch.softmax(x, dim=0)
    print(f"æ ‡å‡†softmax: {standard_softmax}")
    
    # åœ¨çº¿softmaxæ¨¡æ‹Ÿ
    print("\nåœ¨çº¿softmaxè®¡ç®—è¿‡ç¨‹:")
    m = float('-inf')  # è¿è¡Œæœ€å¤§å€¼
    l = 0.0           # è¿è¡Œå’Œ
    result = torch.zeros_like(x)
    
    for i, val in enumerate(x):
        print(f"\næ­¥éª¤ {i+1}: å¤„ç† x[{i}] = {val:.1f}")
        
        # æ›´æ–°æœ€å¤§å€¼
        m_new = max(m, val.item())
        print(f"  æ›´æ–°æœ€å¤§å€¼: m = {m:.1f} -> {m_new:.1f}")
        
        # è®¡ç®—ä¿®æ­£å› å­
        alpha = torch.exp(torch.tensor(m - m_new))
        print(f"  ä¿®æ­£å› å­: Î± = exp({m:.1f} - {m_new:.1f}) = {alpha:.4f}")
        
        # æ›´æ–°è¿è¡Œå’Œ
        l = l * alpha + torch.exp(val - m_new)
        print(f"  æ›´æ–°è¿è¡Œå’Œ: l = {l:.4f}")
        
        # æ›´æ–°ç»“æœ
        result = result * alpha
        result[i] = torch.exp(val - m_new)
        print(f"  å½“å‰ç»“æœ: {result}")
        
        m = m_new
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    result = result / l
    print(f"\næœ€ç»ˆç»“æœ: {result}")
    print(f"æ ‡å‡†ç»“æœ: {standard_softmax}")
    print(f"å·®å¼‚: {torch.abs(result - standard_softmax).max():.6f}")

def test_flash_attention_simple():
    """ç®€åŒ–çš„Flash Attentionæµ‹è¯•"""
    print("\n" + "ğŸš€" * 20)
    print("Flash Attention å®é™…æµ‹è¯•")
    print("ğŸš€" * 20)
    
    # å¯¼å…¥åŸå§‹æ¨¡å—
    try:
        from flash_attention import TritonAttention
        print("âœ… æˆåŠŸå¯¼å…¥TritonAttention")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿flash_attention.pyåœ¨tritonç›®å½•ä¸­")
        return
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦CUDAæ”¯æŒ")
        return
    
    print(f"âœ… CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    
    # æµ‹è¯•å‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°çš„å‚æ•°é¿å…å†…å­˜é—®é¢˜ï¼‰
    BATCH_SIZE = 1
    NUM_HEADS = 2
    SEQ_LEN = 64  # å¾ˆå°çš„åºåˆ—é•¿åº¦
    HEAD_DIM = 32  # å¾ˆå°çš„å¤´ç»´åº¦
    dtype = torch.float32  # ä½¿ç”¨float32é¿å…ç²¾åº¦é—®é¢˜
    
    print_debug(f"æµ‹è¯•å‚æ•°: B={BATCH_SIZE}, H={NUM_HEADS}, S={SEQ_LEN}, D={HEAD_DIM}")
    
    # åˆ›å»ºè¾“å…¥
    print_debug("åˆ›å»ºè¾“å…¥å¼ é‡...")
    torch.manual_seed(42)  # å›ºå®šéšæœºç§å­ä¾¿äºå¤ç°
    Q = torch.randn((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda", requires_grad=True)
    K = torch.randn((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda", requires_grad=True)
    V = torch.randn((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda", requires_grad=True)
    
    print_tensor_info("Q", Q)
    print_tensor_info("K", K)
    print_tensor_info("V", V)
    
    softmax_scale = 1 / (HEAD_DIM**0.5)
    print_debug(f"softmax_scale = {softmax_scale:.6f}")
    
    # æµ‹è¯•éå› æœæ³¨æ„åŠ›
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•: éå› æœæ³¨æ„åŠ›")
    print("="*50)
    
    try:
        print_debug("è®¡ç®—å‚è€ƒå®ç°...")
        P_ref = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale
        P_ref = torch.softmax(P_ref, dim=-1)
        ref_O = torch.matmul(P_ref, V)
        print_tensor_info("å‚è€ƒè¾“å‡º", ref_O)
        
        print_debug("è®¡ç®—Flash Attentionå®ç°...")
        tri_O = TritonAttention.apply(Q, K, V, False, softmax_scale)
        print_tensor_info("Flash Attentionè¾“å‡º", tri_O)
        
        # æ¯”è¾ƒç»“æœ
        diff = torch.abs(ref_O - tri_O)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print_debug(f"ç»“æœæ¯”è¾ƒ:")
        print_debug(f"  æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print_debug(f"  å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        if max_diff < 1e-3:
            print("âœ… éå› æœæ³¨æ„åŠ›æµ‹è¯•é€šè¿‡!")
        else:
            print("âš ï¸ éå› æœæ³¨æ„åŠ›æµ‹è¯•æœ‰å·®å¼‚ï¼Œä½†è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„æ•°å€¼è¯¯å·®")
            
    except Exception as e:
        print(f"âŒ éå› æœæ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•å› æœæ³¨æ„åŠ›
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•: å› æœæ³¨æ„åŠ›")
    print("="*50)
    
    try:
        print_debug("è®¡ç®—å› æœæ³¨æ„åŠ›å‚è€ƒå®ç°...")
        MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device="cuda"))
        P_ref = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale
        P_ref[:, :, MASK == 0] = float("-inf")
        P_ref = torch.softmax(P_ref, dim=-1)
        ref_O = torch.matmul(P_ref, V)
        print_tensor_info("å› æœå‚è€ƒè¾“å‡º", ref_O)
        
        print_debug("è®¡ç®—å› æœFlash Attentionå®ç°...")
        tri_O = TritonAttention.apply(Q, K, V, True, softmax_scale)
        print_tensor_info("å› æœFlash Attentionè¾“å‡º", tri_O)
        
        # æ¯”è¾ƒç»“æœ
        diff = torch.abs(ref_O - tri_O)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()
        
        print_debug(f"å› æœæ³¨æ„åŠ›ç»“æœæ¯”è¾ƒ:")
        print_debug(f"  æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print_debug(f"  å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        if max_diff < 1e-3:
            print("âœ… å› æœæ³¨æ„åŠ›æµ‹è¯•é€šè¿‡!")
        else:
            print("âš ï¸ å› æœæ³¨æ„åŠ›æµ‹è¯•æœ‰å·®å¼‚ï¼Œä½†è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„æ•°å€¼è¯¯å·®")
            
    except Exception as e:
        print(f"âŒ å› æœæ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")

def main():
    print("ğŸ“ Flash Attention æ·±åº¦å­¦ä¹ æ•™ç¨‹")
    print("=" * 60)
    print("è¿™ä¸ªæ•™ç¨‹å°†å¸®åŠ©ä½ ç†è§£Flash Attentionçš„æ ¸å¿ƒæ¦‚å¿µ")
    print("=" * 60)
    
    try:
        # 1. è§£é‡Šæ¦‚å¿µ
        explain_flash_attention_concepts()
        
        # 2. æ¼”ç¤ºåœ¨çº¿softmax
        demonstrate_online_softmax()
        
        # 3. å®é™…æµ‹è¯•
        test_flash_attention_simple()
        
        print("\n" + "ğŸ‰" * 20)
        print("Flash Attentionå­¦ä¹ å®Œæˆ!")
        print("ğŸ‰" * 20)
        
        print("\nğŸ“š å­¦ä¹ æ€»ç»“:")
        print("1. Flash Attentioné€šè¿‡åˆ†å—è®¡ç®—é¿å…äº†O(NÂ²)å†…å­˜å¤æ‚åº¦")
        print("2. åœ¨çº¿softmaxç®—æ³•æ˜¯æ ¸å¿ƒåˆ›æ–°ï¼Œå…è®¸æµå¼å¤„ç†")
        print("3. ä¿®æ­£å› å­Î±ç¡®ä¿äº†æ•°å€¼ç¨³å®šæ€§å’Œæ­£ç¡®æ€§")
        print("4. GPUå†…å­˜å±‚æ¬¡ç»“æ„ä¼˜åŒ–æ˜¯æ€§èƒ½æå‡çš„å…³é”®")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
