"""
Flash Attention å¯è§†åŒ–å·¥å…·

è¿™ä¸ªæ¨¡å—æä¾›äº†å¯è§†åŒ–Flash Attentionè®¡ç®—è¿‡ç¨‹çš„å·¥å…·ï¼Œ
å¸®åŠ©ç†è§£åˆ†å—è®¡ç®—å’Œåœ¨çº¿softmaxç®—æ³•ã€‚
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Optional, Tuple


def visualize_attention_matrix(attention_scores: torch.Tensor, 
                             title: str = "Attention Matrix",
                             save_path: Optional[str] = None,
                             show_colorbar: bool = True):
    """
    å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µ
    
    Args:
        attention_scores: æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ [seq_len, seq_len]
        title: å›¾è¡¨æ ‡é¢˜
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        show_colorbar: æ˜¯å¦æ˜¾ç¤ºé¢œè‰²æ¡
    """
    plt.figure(figsize=(10, 8))
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(attention_scores, torch.Tensor):
        attention_scores = attention_scores.detach().cpu().numpy()
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    sns.heatmap(attention_scores, 
                cmap='Blues', 
                cbar=show_colorbar,
                square=True,
                xticklabels=False,
                yticklabels=False)
    
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('Key Position', fontsize=12)
    plt.ylabel('Query Position', fontsize=12)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()


def visualize_causal_mask(seq_len: int, 
                         title: str = "Causal Attention Mask"):
    """
    å¯è§†åŒ–å› æœæ³¨æ„åŠ›æ©ç 
    
    Args:
        seq_len: åºåˆ—é•¿åº¦
        title: å›¾è¡¨æ ‡é¢˜
    """
    # åˆ›å»ºä¸‹ä¸‰è§’æ©ç 
    mask = torch.tril(torch.ones(seq_len, seq_len))
    
    plt.figure(figsize=(8, 8))
    sns.heatmap(mask.numpy(), 
                cmap='RdYlBu_r', 
                cbar=True,
                square=True,
                xticklabels=False,
                yticklabels=False,
                vmin=0, vmax=1)
    
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('Key Position', fontsize=12)
    plt.ylabel('Query Position', fontsize=12)
    
    # æ·»åŠ è¯´æ˜
    plt.text(seq_len//2, -seq_len//10, 
             'White: Can Attend, Blue: Masked', 
             ha='center', fontsize=10)
    
    plt.show()


def visualize_block_computation(seq_len: int, 
                              block_size_q: int, 
                              block_size_kv: int,
                              title: str = "Flash Attention Block Computation"):
    """
    å¯è§†åŒ–Flash Attentionçš„åˆ†å—è®¡ç®—è¿‡ç¨‹
    
    Args:
        seq_len: åºåˆ—é•¿åº¦
        block_size_q: Qå—å¤§å°
        block_size_kv: K/Vå—å¤§å°
        title: å›¾è¡¨æ ‡é¢˜
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. QçŸ©é˜µåˆ†å—
    q_blocks = torch.zeros(seq_len, seq_len)
    for i in range(0, seq_len, block_size_q):
        end_i = min(i + block_size_q, seq_len)
        q_blocks[i:end_i, :] = (i // block_size_q) + 1
    
    sns.heatmap(q_blocks.numpy(), ax=axes[0], cmap='Set3', 
                cbar=True, square=True, xticklabels=False, yticklabels=False)
    axes[0].set_title('Q Matrix Blocks', fontweight='bold')
    axes[0].set_xlabel('Head Dimension')
    axes[0].set_ylabel('Sequence Position')
    
    # 2. K/VçŸ©é˜µåˆ†å—
    kv_blocks = torch.zeros(seq_len, seq_len)
    for j in range(0, seq_len, block_size_kv):
        end_j = min(j + block_size_kv, seq_len)
        kv_blocks[:, j:end_j] = (j // block_size_kv) + 1
    
    sns.heatmap(kv_blocks.numpy(), ax=axes[1], cmap='Set2', 
                cbar=True, square=True, xticklabels=False, yticklabels=False)
    axes[1].set_title('K/V Matrix Blocks', fontweight='bold')
    axes[1].set_xlabel('Sequence Position')
    axes[1].set_ylabel('Head Dimension')
    
    # 3. æ³¨æ„åŠ›è®¡ç®—æ¨¡å¼
    attention_pattern = torch.zeros(seq_len, seq_len)
    for i in range(0, seq_len, block_size_q):
        for j in range(0, seq_len, block_size_kv):
            end_i = min(i + block_size_q, seq_len)
            end_j = min(j + block_size_kv, seq_len)
            block_id = (i // block_size_q) * (seq_len // block_size_kv) + (j // block_size_kv)
            attention_pattern[i:end_i, j:end_j] = block_id + 1
    
    sns.heatmap(attention_pattern.numpy(), ax=axes[2], cmap='viridis', 
                cbar=True, square=True, xticklabels=False, yticklabels=False)
    axes[2].set_title('Attention Computation Order', fontweight='bold')
    axes[2].set_xlabel('Key Position')
    axes[2].set_ylabel('Query Position')
    
    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()


def compare_attention_implementations(Q: torch.Tensor, 
                                    K: torch.Tensor, 
                                    V: torch.Tensor,
                                    causal: bool = False,
                                    head_idx: int = 0,
                                    batch_idx: int = 0):
    """
    æ¯”è¾ƒæ ‡å‡†æ³¨æ„åŠ›å’ŒFlash Attentionçš„ç»“æœ
    
    Args:
        Q, K, V: è¾“å…¥å¼ é‡
        causal: æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
        head_idx: è¦å¯è§†åŒ–çš„å¤´ç´¢å¼•
        batch_idx: è¦å¯è§†åŒ–çš„æ‰¹æ¬¡ç´¢å¼•
    """
    seq_len = Q.shape[2]
    head_dim = Q.shape[3]
    softmax_scale = 1 / (head_dim ** 0.5)
    
    # æ ‡å‡†å®ç°
    print("ğŸ” è®¡ç®—æ ‡å‡†æ³¨æ„åŠ›å®ç°...")
    P_std = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale
    
    if causal:
        mask = torch.tril(torch.ones(seq_len, seq_len, device=Q.device))
        P_std[:, :, mask == 0] = float('-inf')
    
    P_std = torch.softmax(P_std.float(), dim=-1).to(V.dtype)  # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
    O_std = torch.matmul(P_std, V)
    
    # Flash Attentionå®ç°
    print("âš¡ è®¡ç®—Flash Attentionå®ç°...")
    from flash_attention_debug import TritonAttentionDebug
    O_flash = TritonAttentionDebug.apply(Q, K, V, causal, softmax_scale)
    
    # æå–å•ä¸ªå¤´çš„ç»“æœè¿›è¡Œå¯è§†åŒ–
    P_vis = P_std[batch_idx, head_idx].detach().cpu()
    O_std_vis = O_std[batch_idx, head_idx].detach().cpu()
    O_flash_vis = O_flash[batch_idx, head_idx].detach().cpu()
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ç¬¬ä¸€è¡Œï¼šæ³¨æ„åŠ›çŸ©é˜µå’Œè¾“å‡º
    sns.heatmap(P_vis.numpy(), ax=axes[0, 0], cmap='Blues', 
                cbar=True, square=True, xticklabels=False, yticklabels=False)
    axes[0, 0].set_title('Attention Weights', fontweight='bold')
    
    sns.heatmap(O_std_vis.numpy(), ax=axes[0, 1], cmap='viridis', 
                cbar=True, xticklabels=False, yticklabels=False)
    axes[0, 1].set_title('Standard Attention Output', fontweight='bold')
    
    sns.heatmap(O_flash_vis.numpy(), ax=axes[0, 2], cmap='viridis', 
                cbar=True, xticklabels=False, yticklabels=False)
    axes[0, 2].set_title('Flash Attention Output', fontweight='bold')
    
    # ç¬¬äºŒè¡Œï¼šå·®å¼‚åˆ†æ
    diff = torch.abs(O_std_vis - O_flash_vis)
    
    axes[1, 0].hist(P_vis.flatten().numpy(), bins=50, alpha=0.7, color='blue')
    axes[1, 0].set_title('Attention Weights Distribution')
    axes[1, 0].set_xlabel('Attention Score')
    axes[1, 0].set_ylabel('Frequency')
    
    axes[1, 1].plot(O_std_vis.mean(dim=1).numpy(), label='Standard', linewidth=2)
    axes[1, 1].plot(O_flash_vis.mean(dim=1).numpy(), label='Flash', linewidth=2, linestyle='--')
    axes[1, 1].set_title('Output Mean by Position')
    axes[1, 1].set_xlabel('Sequence Position')
    axes[1, 1].set_ylabel('Mean Output Value')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    sns.heatmap(diff.numpy(), ax=axes[1, 2], cmap='Reds', 
                cbar=True, xticklabels=False, yticklabels=False)
    axes[1, 2].set_title(f'Absolute Difference\n(Max: {diff.max():.2e})')
    
    plt.suptitle(f'Attention Comparison ({"Causal" if causal else "Non-Causal"})', 
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»“æœç»Ÿè®¡:")
    print(f"   æœ€å¤§å·®å¼‚: {diff.max():.2e}")
    print(f"   å¹³å‡å·®å¼‚: {diff.mean():.2e}")
    print(f"   ç›¸å¯¹è¯¯å·®: {(diff.mean() / O_std_vis.abs().mean()):.2e}")


def create_learning_visualization():
    """
    åˆ›å»ºFlash Attentionå­¦ä¹ å¯è§†åŒ–
    """
    print("ğŸ¨ åˆ›å»ºFlash Attentionå­¦ä¹ å¯è§†åŒ–...")
    
    # 1. å› æœæ©ç å¯è§†åŒ–
    print("\n1ï¸âƒ£ å› æœæ³¨æ„åŠ›æ©ç ")
    visualize_causal_mask(16, "Causal Attention Mask (16x16)")
    
    # 2. åˆ†å—è®¡ç®—å¯è§†åŒ–
    print("\n2ï¸âƒ£ åˆ†å—è®¡ç®—æ¨¡å¼")
    visualize_block_computation(64, 16, 16, "Flash Attention Block Computation (64x64)")
    
    # 3. å°è§„æ¨¡å®é™…æ¯”è¾ƒ
    print("\n3ï¸âƒ£ å®é™…è®¡ç®—æ¯”è¾ƒ")
    torch.manual_seed(42)
    B, H, S, D = 1, 2, 32, 64
    Q = torch.randn(B, H, S, D, device='cuda', dtype=torch.float16)
    K = torch.randn(B, H, S, D, device='cuda', dtype=torch.float16)
    V = torch.randn(B, H, S, D, device='cuda', dtype=torch.float16)
    
    # éå› æœæ³¨æ„åŠ›æ¯”è¾ƒ
    print("\n   ğŸ“ˆ éå› æœæ³¨æ„åŠ›æ¯”è¾ƒ")
    compare_attention_implementations(Q, K, V, causal=False, head_idx=0)
    
    # å› æœæ³¨æ„åŠ›æ¯”è¾ƒ
    print("\n   ğŸ“ˆ å› æœæ³¨æ„åŠ›æ¯”è¾ƒ")
    compare_attention_implementations(Q, K, V, causal=True, head_idx=0)


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        create_learning_visualization()
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¯è§†åŒ–ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install matplotlib seaborn")
