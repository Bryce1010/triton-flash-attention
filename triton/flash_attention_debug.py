import torch
import triton
import triton.language as tl
import numpy as np


def print_debug(msg, *args):
    """Debugæ‰“å°å‡½æ•°"""
    print(f"[DEBUG] {msg}", *args)


def print_tensor_info(name, tensor):
    """æ‰“å°å¼ é‡ä¿¡æ¯"""
    if tensor is not None:
        print(f"[TENSOR] {name}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}")
        if tensor.numel() < 20:  # åªæœ‰å°å¼ é‡æ‰æ‰“å°å…·ä½“å€¼
            print(f"         values={tensor.flatten()}")
        else:
            print(f"         min={tensor.min().item():.4f}, max={tensor.max().item():.4f}, mean={tensor.mean().item():.4f}")


@triton.jit
def _attn_fwd_inner_debug(
    O_block,
    l_i,
    m_i,
    Q_block,
    K_block_ptr,
    V_block_ptr,
    block_index_q,
    softmax_scale,
    BLOCK_SIZE_Q: tl.constexpr,
    BLOCK_SIZE_KV: tl.constexpr,
    STAGE: tl.constexpr,
    offs_q: tl.constexpr,
    offs_kv: tl.constexpr,
    SEQ_LEN: tl.constexpr,
):
    """å¸¦è°ƒè¯•ä¿¡æ¯çš„å†…éƒ¨å‰å‘ä¼ æ’­å‡½æ•°"""
    
    # ç¡®å®šå¤„ç†èŒƒå›´
    if STAGE == 1:
        # å¯¹è§’çº¿å·¦è¾¹çš„å—ï¼ˆéå› æœæ³¨æ„åŠ›çš„ä¸€éƒ¨åˆ†ï¼‰
        lo, hi = 0, block_index_q * BLOCK_SIZE_Q
    elif STAGE == 2:
        # å¯¹è§’çº¿ä¸Šçš„å—ï¼ˆéœ€è¦æ©ç çš„è¿‡æ¸¡å—ï¼‰
        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q
        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)
    else:
        # éå› æœæ³¨æ„åŠ›ï¼ˆå¤„ç†æ•´ä¸ªåºåˆ—ï¼‰
        lo, hi = 0, SEQ_LEN

    # ç§»åŠ¨Kå’ŒVæŒ‡é’ˆåˆ°æ­£ç¡®ä½ç½®
    K_block_ptr = tl.advance(K_block_ptr, (0, lo))
    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))

    # ä¸»å¾ªç¯ï¼šéå†Kã€Vå—
    for start_kv in range(lo, hi, BLOCK_SIZE_KV):
        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)

        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° QK^T
        K_block = tl.load(K_block_ptr)
        QK_block = tl.dot(Q_block, K_block)

        # 2. åº”ç”¨ç¼©æ”¾å’Œæ©ç 
        if STAGE == 2:
            # å› æœæ©ç ï¼šåªèƒ½çœ‹åˆ°å½“å‰ä½ç½®åŠä¹‹å‰çš„ä½ç½®
            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])
            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)
            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))
            QK_block -= m_ij[:, None]
        else:
            # éå› æœæƒ…å†µï¼šç›´æ¥åº”ç”¨ç¼©æ”¾
            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)
            QK_block = QK_block * softmax_scale - m_ij[:, None]

        # 3. è®¡ç®—softmaxï¼ˆåœ¨çº¿ç®—æ³•ï¼‰
        P_block = tl.math.exp(QK_block)  # exp(QK - max)
        l_ij = tl.sum(P_block, 1)        # å½“å‰å—çš„è¡Œå’Œ

        # 4. æ›´æ–°è¿è¡Œç»Ÿè®¡é‡ï¼ˆFlash Attentionçš„æ ¸å¿ƒï¼‰
        alpha = tl.math.exp(m_i - m_ij)  # ä¿®æ­£å› å­
        l_i = l_i * alpha + l_ij         # æ›´æ–°è¿è¡Œå’Œ
        
        # 5. è®¡ç®—è¾“å‡ºå¹¶ç´¯åŠ 
        V_block = tl.load(V_block_ptr)
        P_block = P_block.to(tl.float16)
        
        # å…³é”®æ­¥éª¤ï¼šO_new = P Ã— V + O_old Ã— alpha
        O_block = O_block * alpha[:, None]
        O_block = tl.dot(P_block, V_block, O_block)

        # 6. æ›´æ–°æœ€å¤§å€¼
        m_i = m_ij

        # 7. ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªKã€Vå—
        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))
        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))
        
    return O_block, l_i, m_i


@triton.autotune(
    [
        triton.Config(
            {"BLOCK_SIZE_Q": BLOCK_SIZE_Q, "BLOCK_SIZE_KV": BLOCK_SIZE_KV},
            num_stages=num_stages,
            num_warps=num_warps,
        )
        for BLOCK_SIZE_Q in [64, 128]
        for BLOCK_SIZE_KV in [32, 64]
        for num_stages in ([3, 4, 7])
        for num_warps in [2, 4]
    ],
    key=["SEQ_LEN", "HEAD_DIM"],
)
@triton.jit
def _attn_fwd_debug(
    Q,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM
    K,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM
    V,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM
    softmax_scale,
    M,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN
    O,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM
    stride_Q_batch,
    stride_Q_head,
    stride_Q_seq,
    stride_Q_dim,
    stride_K_batch,
    stride_K_head,
    stride_K_seq,
    stride_K_dim,
    stride_V_batch,
    stride_V_head,
    stride_V_seq,
    stride_V_dim,
    stride_O_batch,
    stride_O_head,
    stride_O_seq,
    stride_O_dim,
    BATCH_SIZE,
    NUM_HEADS: tl.constexpr,
    SEQ_LEN: tl.constexpr,
    HEAD_DIM: tl.constexpr,
    BLOCK_SIZE_Q: tl.constexpr,
    BLOCK_SIZE_KV: tl.constexpr,
    STAGE: tl.constexpr,
):
    """å¸¦è°ƒè¯•ä¿¡æ¯çš„å‰å‘ä¼ æ’­ä¸»å‡½æ•°"""
    
    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)

    # è·å–å½“å‰å¤„ç†çš„å—ç´¢å¼•
    block_index_q = tl.program_id(0)  # å¤„ç†ç¬¬å‡ ä¸ªQå—
    index_batch_head = tl.program_id(1)  # å¤„ç†ç¬¬å‡ ä¸ªbatchÃ—head
    
    # è§£æbatchå’Œheadç´¢å¼•
    index_batch = index_batch_head // NUM_HEADS
    index_head = index_batch_head % NUM_HEADS

    # è®¡ç®—å†…å­˜åç§»é‡
    qvk_offset = (
        index_batch.to(tl.int64) * stride_Q_batch
        + index_head.to(tl.int64) * stride_Q_head
    )

    # åˆ›å»ºå—æŒ‡é’ˆï¼ˆé«˜æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼ï¼‰
    Q_block_ptr = tl.make_block_ptr(
        base=Q + qvk_offset,
        shape=(SEQ_LEN, HEAD_DIM),
        strides=(stride_Q_seq, stride_Q_dim),
        offsets=(block_index_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )

    V_block_ptr = tl.make_block_ptr(
        base=V + qvk_offset,
        shape=(SEQ_LEN, HEAD_DIM),
        strides=(stride_V_seq, stride_V_dim),
        offsets=(0, 0),
        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),
        order=(1, 0),
    )

    # KçŸ©é˜µéœ€è¦è½¬ç½®ï¼Œæ‰€ä»¥stridesæ˜¯åçš„
    K_block_ptr = tl.make_block_ptr(
        base=K + qvk_offset,
        shape=(HEAD_DIM, SEQ_LEN),
        strides=(stride_K_dim, stride_K_seq),
        offsets=(0, 0),
        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),
        order=(0, 1),
    )

    O_block_ptr = tl.make_block_ptr(
        base=O + qvk_offset,
        shape=(SEQ_LEN, HEAD_DIM),
        strides=(stride_O_seq, stride_O_dim),
        offsets=(block_index_q * BLOCK_SIZE_Q, 0),
        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),
        order=(1, 0),
    )

    # è®¡ç®—åç§»é‡
    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)
    offs_kv = tl.arange(0, BLOCK_SIZE_KV)

    # åˆå§‹åŒ–ç´¯åŠ å™¨ï¼ˆFlash Attentionçš„å…³é”®å˜é‡ï¼‰
    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float("inf")  # è¿è¡Œæœ€å¤§å€¼
    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0           # è¿è¡Œå’Œ
    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)   # è¾“å‡ºç´¯åŠ å™¨

    # åŠ è½½Qå—ï¼ˆåœ¨æ•´ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­ä¿æŒåœ¨SRAMä¸­ï¼‰
    Q_block = tl.load(Q_block_ptr)

    # æ ¹æ®STAGEæ‰§è¡Œä¸åŒçš„è®¡ç®—ç­–ç•¥
    if STAGE == 1 or STAGE == 3:
        # éå› æœæ³¨æ„åŠ› æˆ– å› æœæ³¨æ„åŠ›çš„å·¦åŠéƒ¨åˆ†
        O_block, l_i, m_i = _attn_fwd_inner_debug(
            O_block, l_i, m_i, Q_block, K_block_ptr, V_block_ptr,
            block_index_q, softmax_scale, BLOCK_SIZE_Q, BLOCK_SIZE_KV,
            4 - STAGE, offs_q, offs_kv, SEQ_LEN,
        )

    if STAGE == 3:
        # å› æœæ³¨æ„åŠ›çš„å¯¹è§’çº¿å—ï¼ˆéœ€è¦æ©ç ï¼‰
        O_block, l_i, m_i = _attn_fwd_inner_debug(
            O_block, l_i, m_i, Q_block, K_block_ptr, V_block_ptr,
            block_index_q, softmax_scale, BLOCK_SIZE_Q, BLOCK_SIZE_KV,
            2, offs_q, offs_kv, SEQ_LEN,
        )

    # æœ€ç»ˆå¤„ç†ï¼šè®¡ç®—logsumexpå¹¶å½’ä¸€åŒ–
    m_i += tl.math.log(l_i)  # è®¡ç®—logsumexpï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰
    O_block = O_block / l_i[:, None]  # æœ€ç»ˆå½’ä¸€åŒ–

    # å­˜å‚¨ç»“æœ
    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q
    tl.store(m_ptrs, m_i)
    tl.store(O_block_ptr, O_block.to(O.type.element_ty))


class TritonAttentionDebug(torch.autograd.Function):
    """å¸¦è°ƒè¯•ä¿¡æ¯çš„Triton Attentionç±»"""

    @staticmethod
    def forward(ctx, Q, K, V, causal, softmax_scale):
        print_debug("=== Flash Attention å‰å‘ä¼ æ’­å¼€å§‹ ===")
        print_tensor_info("è¾“å…¥Q", Q)
        print_tensor_info("è¾“å…¥K", K)
        print_tensor_info("è¾“å…¥V", V)
        print_debug(f"causal={causal}, softmax_scale={softmax_scale:.6f}")
        
        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]
        HEAD_DIM_V = V.shape[-1]
        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape

        print_debug(f"çŸ©é˜µç»´åº¦: BATCH_SIZE={BATCH_SIZE}, NUM_HEADS={NUM_HEADS}, SEQ_LEN={SEQ_LEN}, HEAD_DIM={HEAD_DIM}")

        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V

        O = torch.empty_like(Q)
        stage = 3 if causal else 1
        print_debug(f"æ‰§è¡Œæ¨¡å¼: STAGE={stage} ({'å› æœæ³¨æ„åŠ›' if causal else 'éå› æœæ³¨æ„åŠ›'})")

        # è®¡ç®—GPUç½‘æ ¼å¤§å°
        grid = lambda args: (
            triton.cdiv(SEQ_LEN, args["BLOCK_SIZE_Q"]),
            BATCH_SIZE * NUM_HEADS,
            1,
        )

        # Må­˜å‚¨logsumexpå€¼ï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰
        M = torch.empty(
            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32
        )
        print_debug(f"åˆ›å»ºlogsumexpçŸ©é˜µM: {M.shape}")

        print_debug("å¼€å§‹æ‰§è¡ŒTriton kernel...")
        _attn_fwd_debug[grid](
            Q=Q, K=K, V=V, softmax_scale=softmax_scale, M=M, O=O,
            stride_Q_batch=Q.stride(0), stride_Q_head=Q.stride(1),
            stride_Q_seq=Q.stride(2), stride_Q_dim=Q.stride(3),
            stride_K_batch=K.stride(0), stride_K_head=K.stride(1),
            stride_K_seq=K.stride(2), stride_K_dim=K.stride(3),
            stride_V_batch=V.stride(0), stride_V_head=V.stride(1),
            stride_V_seq=V.stride(2), stride_V_dim=V.stride(3),
            stride_O_batch=O.stride(0), stride_O_head=O.stride(1),
            stride_O_seq=O.stride(2), stride_O_dim=O.stride(3),
            BATCH_SIZE=Q.shape[0], NUM_HEADS=Q.shape[1],
            SEQ_LEN=Q.shape[2], HEAD_DIM=HEAD_DIM_K, STAGE=stage,
        )

        print_tensor_info("è¾“å‡ºO", O)
        print_tensor_info("logsumexp M", M)
        print_debug("=== Flash Attention å‰å‘ä¼ æ’­å®Œæˆ ===\n")

        ctx.save_for_backward(Q, K, V, O, M)
        ctx.grid = grid
        ctx.softmax_scale = softmax_scale
        ctx.HEAD_DIM = HEAD_DIM_K
        ctx.causal = causal
        return O

    @staticmethod
    def backward(ctx, dO):
        print_debug("=== Flash Attention åå‘ä¼ æ’­å¼€å§‹ ===")
        Q, K, V, O, M = ctx.saved_tensors
        print_tensor_info("æ¢¯åº¦dO", dO)

        assert dO.is_contiguous()
        assert Q.stride() == K.stride() == V.stride() == O.stride() == dO.stride()
        
        dQ = torch.empty_like(Q)
        dK = torch.empty_like(K)
        dV = torch.empty_like(V)

        BATCH_SIZE, NUM_HEADS, SEQ_LEN = Q.shape[:3]
        NUM_WARPS, NUM_STAGES = 4, 3
        BLOCK_SIZE_MICRO, BLOCK_SIZE_MACRO = 32, 128

        print_debug(f"åå‘ä¼ æ’­å‚æ•°: MICRO={BLOCK_SIZE_MICRO}, MACRO={BLOCK_SIZE_MACRO}")

        # é¢„å¤„ç†ï¼šè®¡ç®—D = rowsum(dO âŠ™ O)
        preprocess_grid = (SEQ_LEN // BLOCK_SIZE_MACRO, BATCH_SIZE * NUM_HEADS)
        D = torch.empty_like(M)
        print_debug("æ­¥éª¤1: è®¡ç®—DçŸ©é˜µï¼ˆé¢„å¤„ç†ï¼‰...")

        # ä½¿ç”¨åŸå§‹æ¨¡å—çš„åå‘ä¼ æ’­å‡½æ•°
        import importlib.util
        import os
        
        # åŠ¨æ€å¯¼å…¥flash_attentionæ¨¡å—
        flash_attention_path = os.path.join(os.path.dirname(__file__), 'flash_attention.py')
        spec = importlib.util.spec_from_file_location("flash_attention", flash_attention_path)
        flash_attention = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(flash_attention)
        
        _attn_bwd_preprocess = flash_attention._attn_bwd_preprocess
        _attn_bwd_dk_dv = flash_attention._attn_bwd_dk_dv
        _attn_bwd_dq = flash_attention._attn_bwd_dq
        _attn_bwd_preprocess[preprocess_grid](
            O=O, dO=dO, D=D, SEQ_LEN=SEQ_LEN,
            BLOCK_SIZE_Q=BLOCK_SIZE_MACRO, HEAD_DIM=ctx.HEAD_DIM,
        )

        print_tensor_info("DçŸ©é˜µ", D)

        grid = (SEQ_LEN // BLOCK_SIZE_MACRO, 1, BATCH_SIZE * NUM_HEADS)
        stage = 3 if ctx.causal else 1

        print_debug("æ­¥éª¤2: è®¡ç®—dKå’ŒdV...")
        _attn_bwd_dk_dv[grid](
            Q=Q, K=K, V=V, softmax_scale=ctx.softmax_scale, dO=dO,
            dQ=dQ, dK=dK, dV=dV, M=M, D=D,
            stride_batch=Q.stride(0), stride_head=Q.stride(1),
            stride_seq=Q.stride(2), stride_dim=Q.stride(3),
            NUM_HEADS=NUM_HEADS, SEQ_LEN=SEQ_LEN,
            BLOCK_Q=BLOCK_SIZE_MICRO, BLOCK_KV=BLOCK_SIZE_MACRO,
            HEAD_DIM=ctx.HEAD_DIM, STAGE=stage,
            num_warps=NUM_WARPS, num_stages=NUM_STAGES,
        )

        print_debug("æ­¥éª¤3: è®¡ç®—dQ...")
        _attn_bwd_dq[grid](
            Q=Q, K=K, V=V, softmax_scale=ctx.softmax_scale, dO=dO,
            dQ=dQ, dK=dK, dV=dV, M=M, D=D,
            stride_batch=Q.stride(0), stride_head=Q.stride(1),
            stride_seq=Q.stride(2), stride_dim=Q.stride(3),
            NUM_HEADS=NUM_HEADS, SEQ_LEN=SEQ_LEN,
            BLOCK_Q=BLOCK_SIZE_MACRO, BLOCK_KV=BLOCK_SIZE_MICRO,
            HEAD_DIM=ctx.HEAD_DIM, STAGE=stage,
            num_warps=NUM_WARPS, num_stages=NUM_STAGES,
        )

        print_tensor_info("æ¢¯åº¦dQ", dQ)
        print_tensor_info("æ¢¯åº¦dK", dK)
        print_tensor_info("æ¢¯åº¦dV", dV)
        print_debug("=== Flash Attention åå‘ä¼ æ’­å®Œæˆ ===\n")

        return dQ, dK, dV, None, None


def test_debug_attention():
    """æµ‹è¯•debugç‰ˆæœ¬çš„Flash Attention"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•Flash Attention (Debugç‰ˆæœ¬)")
    print("=" * 60)
    
    # ä½¿ç”¨è¾ƒå°çš„å‚æ•°ä¾¿äºè§‚å¯Ÿ
    BATCH_SIZE = 2
    NUM_HEADS = 4
    SEQ_LEN = 256  # è¾ƒå°çš„åºåˆ—é•¿åº¦
    HEAD_DIM = 64
    dtype = torch.float16

    print_debug(f"æµ‹è¯•å‚æ•°: B={BATCH_SIZE}, H={NUM_HEADS}, S={SEQ_LEN}, D={HEAD_DIM}")

    # åˆ›å»ºè¾“å…¥å¼ é‡
    Q = torch.empty((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda").normal_(mean=0.0, std=0.5).requires_grad_()
    K = torch.empty((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda").normal_(mean=0.0, std=0.5).requires_grad_()
    V = torch.empty((BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device="cuda").normal_(mean=0.0, std=0.5).requires_grad_()

    softmax_scale = 1 / (HEAD_DIM**0.5)
    dO = torch.randn_like(Q)

    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•1: éå› æœæ³¨æ„åŠ›")
    print("="*60)
    
    # æµ‹è¯•éå› æœæ³¨æ„åŠ›
    causal = False
    
    # å‚è€ƒå®ç°
    print_debug("è®¡ç®—å‚è€ƒå®ç°...")
    P_ref = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale
    P_ref = torch.softmax(P_ref.float(), dim=-1).to(V.dtype)
    ref_O = torch.matmul(P_ref, V)
    print_tensor_info("å‚è€ƒè¾“å‡º", ref_O)

    # Flash Attentionå®ç°
    print_debug("è®¡ç®—Flash Attentionå®ç°...")
    tri_O = TritonAttentionDebug.apply(Q, K, V, causal, softmax_scale).half()

    # æ¯”è¾ƒç»“æœ
    diff = torch.abs(ref_O - tri_O)
    max_diff = diff.max().item()
    mean_diff = diff.mean().item()
    
    print_debug(f"ç»“æœæ¯”è¾ƒ: max_diff={max_diff:.6f}, mean_diff={mean_diff:.6f}")
    
    if max_diff < 1e-2:
        print("âœ… éå› æœæ³¨æ„åŠ›æµ‹è¯•é€šè¿‡!")
    else:
        print("âŒ éå› æœæ³¨æ„åŠ›æµ‹è¯•å¤±è´¥!")

    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•2: å› æœæ³¨æ„åŠ›")
    print("="*60)
    
    # æµ‹è¯•å› æœæ³¨æ„åŠ›
    causal = True
    
    # å‚è€ƒå®ç°
    print_debug("è®¡ç®—å› æœæ³¨æ„åŠ›å‚è€ƒå®ç°...")
    MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device="cuda"))
    P_ref = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale
    P_ref[:, :, MASK == 0] = float("-inf")
    P_ref = torch.softmax(P_ref.float(), dim=-1).to(V.dtype)
    ref_O = torch.matmul(P_ref, V)
    print_tensor_info("å› æœæ³¨æ„åŠ›å‚è€ƒè¾“å‡º", ref_O)

    # Flash Attentionå®ç°
    print_debug("è®¡ç®—å› æœFlash Attentionå®ç°...")
    tri_O = TritonAttentionDebug.apply(Q, K, V, causal, softmax_scale).half()

    # æ¯”è¾ƒç»“æœ
    diff = torch.abs(ref_O - tri_O)
    max_diff = diff.max().item()
    mean_diff = diff.mean().item()
    
    print_debug(f"å› æœæ³¨æ„åŠ›ç»“æœæ¯”è¾ƒ: max_diff={max_diff:.6f}, mean_diff={mean_diff:.6f}")
    
    if max_diff < 1e-2:
        print("âœ… å› æœæ³¨æ„åŠ›æµ‹è¯•é€šè¿‡!")
    else:
        print("âŒ å› æœæ³¨æ„åŠ›æµ‹è¯•å¤±è´¥!")

    print("\n" + "="*60)
    print("ğŸ¯ Flash Attention æ ¸å¿ƒæ¦‚å¿µæ€»ç»“")
    print("="*60)
    print("1. ğŸ§© åˆ†å—è®¡ç®—: å°†å¤§çŸ©é˜µåˆ†æˆå°å—ï¼Œé€å—å¤„ç†")
    print("2. ğŸ“Š åœ¨çº¿softmax: ä½¿ç”¨è¿è¡Œæœ€å¤§å€¼m_iå’Œè¿è¡Œå’Œl_i")
    print("3. ğŸ”„ ä¿®æ­£å› å­Î±: exp(m_old - m_new)ç”¨äºæ›´æ–°ä¹‹å‰çš„ç»“æœ")
    print("4. ğŸ’¾ å†…å­˜æ•ˆç‡: é¿å…å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ")
    print("5. âš¡ GPUä¼˜åŒ–: åˆ©ç”¨SRAMå‡å°‘å†…å­˜è®¿é—®")


if __name__ == "__main__":
    test_debug_attention()
